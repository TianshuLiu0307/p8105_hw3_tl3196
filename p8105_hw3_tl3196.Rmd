---
title: "p8105_hw3_tl3196"
author: "Tianshu Liu"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.height = 6, fig.width = 10)
```

```{r library}
library(tidyverse)
library(ggplot2)
library(patchwork)
```


## Problem 1
```{r import_instacart}
library(p8105.datasets)
data("instacart")
```

The instacart data set is ``r nrow(instacart)``×``r ncol(instacart)``, with each row resprenting a single product from an instacart order.

The variables contain ``r colnames(instacart)``, the data type in each variable is shown in the table below:

```{r column_info, echo=FALSE}
col_names = colnames(instacart)
col_types = c()
for (col in col_names){
  col_types = c(col_types, class(pull(instacart, col)))
}

knitr::kable(
  tibble(col_names, col_types)
)
```


```{r aisle_count}
aisle_count = 
instacart %>%
  group_by(aisle) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

aisle_count
```
The instacart data set contains ``r nrow(aisle_count)`` aisles.

Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. 

```{r aisle_plot}
aisle_count %>%
  filter(n > 10000) %>%
  ggplot(aes(x = reorder(aisle, -n), y = n)) +
  geom_point()+
  labs(
    x = "Aisles",
    y = "Number of Items",
    title = "Number of Items Ordered in the Top Aisles",
    caption = "@instacart"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) 
```

Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r top_3_in_baking}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  select(product_name, aisle) %>% 
  group_by(aisle, product_name) %>%
  summarise(
    n_product = n()
  ) %>% 
  mutate(
    product_rank = min_rank(desc(n_product))
  ) %>% 
  filter(product_rank < 4) %>% 
  arrange(aisle, product_rank)
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week:

```{r mean_order_hour}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(
    mean_order_hour = mean(order_hour_of_day)
  ) %>% 
  mutate(
    order_dow = factor(
      order_dow,levels=0:6,
      labels=c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_order_hour
  )
```

# Problem 2

Import data from `accel_data.csv`.

```{r import_accel}
accel_df = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = activity_1:activity_1440,
    names_prefix = "activity_",
    names_to = "minute_of_day",
    names_transform = list(minute_of_day = as.integer),
    values_to = "n_activity",
    values_transform = list(n_activity = as.integer)
  ) %>% 
  mutate(
    weekday_vs_weekend = as.factor(ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday")),
    week = as.integer(week),
    day_id = as.integer(day_id)
  ) %>%
  relocate(weekday_vs_weekend, .after = day)

accel_df
```

This data set records five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF).

This data set contains ``r ncol(accel_df)`` variables, which are ``r colnames(accel_df)``. The variables' data types and description are shown in the table below:

```{r accel_col_info, echo=FALSE}
col_names = colnames(accel_df)
col_types = c()
for (col in col_names){
  col_types = c(col_types, class(pull(accel_df, col)))
}
description = c(
  "which week in the obervation period",
  "which day in the observation period",
  "what day is it (eg. Monday, Tuesday, etc.)",
  "it is on weekday or weekend",
  "the minute of a day",
  "number of activities happeded in this minute"
)
knitr::kable(
  tibble(col_names, col_types, description)
)
```

There are ``r nrow(accel_df)`` rows of observations in the data set.

Then, analyze the accelerometer data focus on the total activity over the day.

```{r day_total_act}
day_total_act = 
  accel_df %>%
  group_by(day_id, day, weekday_vs_weekend) %>% 
  summarise(
    day_total_activity = sum(n_activity)
  )

day_total_act

day_total_act %>% 
  ggplot(aes(x = day_id, y = day_total_activity))+
  geom_point(color = "red", alpha = .5) +
  geom_line(color = "red") +
  labs(
    x = "day id",
    y = "day total activity",
    title = "Total activities over the day"
  )+
  theme_minimal()
```

From the table and line-plot, there is not apparent trend of the total activities over each day in the observation period.

Generate 1440-minute activity time course graph for each day in the observation period.

```{r 24_hour_0}
accel_df %>% 
  mutate(
    day_id = as.character(day_id),
    week = as.character(week)
  ) %>% 
  ggplot(aes(x = minute_of_day, y = n_activity, color = day_id)) +
  labs(
    x = "Minute of Day",
    y = "Number of Activities",
    title = "1440-minute activity time courses for each day"
  ) +
  geom_point(size = .5, alpha = .2) + 
  theme_minimal()+
  theme(legend.position = "bottom")
```

There are too many points to find any explicit conclusions about the activities. Use `facet_grid` function to generate separate graphs for each day. 

```{r 24_hour}
accel_df %>% 
  mutate(
    day_id = as.character(day_id),
    week = as.character(week)
  ) %>% 
  ggplot(aes(x = minute_of_day, y = n_activity)) +
  labs(
    x = "Minute of Day",
    y = "Number of Activities",
    title = "1440-minute activity time courses for each day"
  ) +
  geom_point(size = .1, alpha = .2, color = "red") + 
  geom_line(size = .1, alpha = .5) +
  theme_minimal() +
  facet_grid(cols = vars(day), rows = vars(week))+
  theme(axis.text.x = element_text(size = 4)) 
```

Since there are too many points in each day's graph, it is not clear enough to find data patterns from the messy points and lines. To represent the number of activity in each hour, calculate the mean of n_activity grouped by each hour in each day, and plot the graph again.

```{r 24_hour_2}
accel_df %>% 
  mutate(
    day_id = as.character(day_id),
    week = as.character(week),
    hour = floor(minute_of_day / 60)
  ) %>% 
  group_by(week, day_id, day, hour) %>% 
  summarise(
    mean_activity = mean(n_activity)
  ) %>% 
  ggplot(aes(x = hour, y = mean_activity)) +
  labs(
    x = "Hour of Day",
    y = "Number of Activities",
    title = "24-hour activity time courses for each day"
  ) +
  geom_point(size = .5, alpha = .5, color = "red") + 
  geom_line(size = .5, alpha = .5) +
  theme_minimal() +
  facet_grid(cols = vars(day), rows = vars(week))+
  theme(axis.text.x = element_text(size = 4)) 
```

From the two graphs above, we can find some patterns and conclusions: 

* The number of activity apparently increases after 5 or 6 am, when the patient may get up every day and thus have more motions, decreases at around 8 pm, fluctuates at noon, then increases again in the afternoon, and finally decreases to around 0 after 10 pm, when the patient may fall asleep.
* The patient's most active period of time of each day is between 8 am to 10 pm.
* The peak of activity counts often appears at around 8 am in the morning, and at around 8 pm in the evening.
The peak of activity numbers always appears at around 8 pm on each Friday and Monday, at around 12 pm on Sunday.
* The patient had fewer activities than normal on the 2nd, 5th, 6th, 7th, 22nd, 24th, 31st, 32nd day, which indicates that he might feel sick on these days or he didn't take the accelerometer appropriately.
* In the 2nd and 3rd week, which contains relatively normal data of activity, the patient's activity is relatively fixed in each days of each week.

# Problem 3

Import data from p8105 datasets.

```{r import_ny_noaa}
#Import data from p8105

library(p8105.datasets)
data("ny_noaa")
```

This data contains important weather variables collected by all New York state weather stations from January 1, 1981 through December 31, 2010.

This data set is ``r nrow(ny_noaa)``×``r ncol(ny_noaa)``.

It contains ``r ncol(ny_noaa)`` variables, which are ``r colnames(ny_noaa)``.
The variables' types in the original data set and descriptions are shown in the table below:

```{r noaa_col_info, echo=FALSE}
col_names = colnames(ny_noaa)
col_types = c()
for (col in col_names){
  col_types = c(col_types, class(pull(ny_noaa, col)))
}
description = c(
  "Weather station id",
  "Date of observation",
  "Precipitaion (mm)",
  "Snowfall (mm)",
  "Snow depth (mm)",
  "Maximum temperature (degree C)",
  "Minimum temperature (degree C)"
)
knitr::kable(
  tibble(col_names, col_types, description)
)
```

Though the data set contains ``r nrow(ny_noaa)`` rows of observations, ``r nrow(ny_noaa[rowSums(is.na(ny_noaa))>0,])`` rows of observations in total contain `NA` data, which means ``r scales::percent(nrow(ny_noaa[rowSums(is.na(ny_noaa))>0,])/nrow(ny_noaa), 0.01)`` of the observations contained in this data set have `NA` data.

The table below shows the number and proportion of `NA` in each columns:

```{r na_table, echo = FALSE}
n_total = nrow(ny_noaa)
col_names = colnames(ny_noaa)
n_na = c()
percent = c()
for (col in col_names){
  col_n_na = nrow(ny_noaa[is.na(pull(ny_noaa, col)), ])
  n_na = c(n_na, col_n_na)
  percent = c(percent, scales::percent(col_n_na/n_total, 0.01))
}

knitr::kable(
  tibble(col_names, n_na, percent)
)
```

Since such significant proportion of NA existing in the data set, the missing values can be considered an issue in data analysis.

Check the frequent values in temperature, precipitation, and snowfall by plotting histograms for each varible.

Before checking whether the units used in the data set is reasonable, let's make a little edit to the original data set.
The current types for variables `tmin` and `tmax` are both `character` in the original data set, which is not convenient in analysis.
Thus, change the type of `tmin` and `tmax` to integer in advance.

```{r}
ny_noaa = 
  ny_noaa %>% 
  mutate(
    tmin = as.integer(tmin),
    tmax = as.integer(tmax)
  )
```

Check the current value characteristics of each variable in the data set.

```{r check}
ny_noaa %>% summary()
```

Max/Min of `tmax` and `tmin` can almost reach ± 400 C, which is impossible for the temperature. 
Thus, the value of `tmax` and `tmin` should be divided by `10`.

The unit of snowfall is reasonable, which don't need converting.

The unit of precipitation should also be divided by `10`.

```{r noaa_cleaning}
ny_noaa = 
  ny_noaa %>% 
  separate(date,
           into = c("year", "month", "day"),
           sep = "-",
           convert = TRUE) %>% 
  mutate(
    tmin = tmin / 10,
    tmax = tmax / 10,
    prcp = prcp / 10
  )
```

```{r most_freq_snow}
ny_noaa %>% 
  drop_na(snow) %>% 
  group_by(snow) %>% 
  summarise(
    snow_count = n()
  ) %>% 
  arrange(-snow_count)
```

The most commonly observed value of snowfall is `0`, since most of the days in New York in a year do not have snowfall. Even in winter, it does not snow every day.

```{r avg_tmax}
ny_noaa %>% 
  drop_na(tmax) %>% 
  filter(month %in% c(1, 7)) %>% 
  mutate(
    year = as.character(year),
    month = ifelse(month == 1, "January", "July")
  ) %>% 
  group_by(id, year, month) %>% 
  summarise(
    avg_tmax = mean(tmax)
  ) %>% 
  ggplot(aes(x = year, y = avg_tmax, group = id)) + 
  geom_point(aes(color = id), alpha = .3) +
  geom_line(aes(color = id), alpha = .2) +
  labs(
    x = "year", 
    y = "average max temperature (°C)",
    title = "Max temperature in January and in July in each station across years"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(size=6, angle = 45, hjust = 1),
        legend.position = "none") +
  facet_grid(cols = vars(month)) 
```

From the graph above, we can find some observable / interpretable structures:

* The max temperature in January fluctuated across years, while the max temperature in July was relatively more stable. 
* The max temperatures measured by different stations were basically convergent in trend.
* The max temperature in January was apparently lower than in July from 1981 to 2012.

```{r tmax_boxplot}
ny_noaa %>% 
  drop_na(tmax) %>% 
  filter(month %in% c(1, 7)) %>% 
  mutate(
    year = as.character(year),
    month = ifelse(month == 1, "January", "July")
  ) %>% 
  group_by(id, year, month) %>% 
  summarise(
    avg_tmax = mean(tmax)
  ) %>% 
  ggplot(aes(x = year, y = avg_tmax)) + 
  geom_boxplot() +
  labs(
    x = "year", 
    y = "average max temperature (°C)",
    title = "Max temperature boxplot in January and in July in each station across years"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(size=6, angle = 45, hjust = 1),
        legend.position = "none") +
  facet_grid(cols = vars(month)) 
```

In the box-plot above, it is clear to find some outliers, such as the mean max temperature in January 1982 measured by `USC00301723` and July 1988 measured by `USC00308962`.

```{r tmax_vs_tmin}
tmin_vs_tmax = 
  ny_noaa %>% 
  drop_na(tmax, tmin) %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  labs(
    title = "tmax vs. tmin",
    x = "tmin (°C)",
    y = "tmax (°C)"
  ) + 
  theme_minimal() + 
  theme(legend.position = "bottom")
```


```{r snow, fig.height=8, fig.width=10}
snowfall = 
  ny_noaa %>% 
  filter(
    snow > 0,
    snow < 100
  ) %>% 
  mutate(
    year = as.character(year)
  ) %>% 
  ggplot(aes(x = snow, color = year)) + 
  geom_density() +
  labs(
    x = "snowfall (mm)",
    y = "density",
    title = "Distribution of snowfall values"
  ) + 
  theme_minimal() + 
  theme(legend.position = "bottom")

tmin_vs_tmax + snowfall

```



